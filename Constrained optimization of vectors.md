We need to minimize some function $f(\textbf x)$ subject to the constraints $\textbf A\textbf x=\textbf b$, where $\textbf A$ is a matrix of size $m\times n$ where $m$ is the number of constraints and $n$ is the number of variables. $\textbf x$ is a vector of size $n$ and $\textbf b$ is a vector of size $m$. 

suppose we find the optimal vector $\textbf x^*$. A feasible change $\textbf y$ in this vector should satisfy the constraints$$\textbf A(\textbf x^*+\textbf y)=\textbf b$$However, since $\textbf A\textbf x^*=\textbf b$, it must be that $\textbf A\textbf y=0$. 

Further, at the optimal point, the moving along a feasible direction should not improve the function. This means that $$f(\textbf x^*+\textbf y)\ge f(\textbf x^*)$$We can thus say that the gradient for all feasible directions must be 0. This means that the slope at that point to any feasible change in direction (for a small change) is 0 - indicating that we are at the constrained minima (optimality)$$\nabla f(\textbf x^*)^T\textbf y=0$$The gradient represents the steepest descent direction. $\textbf y$ is orthogonal to it as it is not in the same direction, since otherwise it would cause a change in the objective function. We can then define a vector $\lambda$ of size $m$ such that $$\nabla f(\textbf x^*)=\textbf A^T\lambda$$this is the vector of **lagrange multipliers**. We thus need to solve for both $\textbf x^*$ and $\lambda$, which have $n$ and $m$ unknowns respectively. However, from the constraint equations, we get $m$ equations, and from the condition $\nabla f(\textbf x^*)=\textbf A^T\lambda$ we get $n$ conditions, meaning that this system of $m+n$ equations can be solved to find the optimum. 