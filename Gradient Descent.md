Gradient descent is an iterative optimization algorithm that optimizes parameters by calculating the gradient of the cost function with the direction of steepest descent, with respect to the model parameters. It moves step by step towards the minima, with the step size determined by the **learning rate**. Slower learning rates allow for more accurate descent, as the loss function is complex and there may be many inflection points/local minima. 

If the error function is given by $E$, we can update the vector of weights $\textbf w$ using the equation$$\textbf w^{\tau+1}=\textbf w^\tau-\eta\nabla E$$Where $\tau$ is the iteration number and $\eta$ is the learning rate of the optimization process. ![[Pasted image 20240921213017.png]]

#### For a [[Simple Linear Regression]]
We have the error function$$E=\frac 1n\sum_{i=1}^n(y_i-(mx_i+c))^2$$where $y_i$ is the actual observation and $m$ is the slope, $c$ is the intercept of the regression. We can initially set the parameters $m$ and $c$ to random values or both to 0. The learning rate $\eta$ controls how much the value of $m$ changes at each step. We calculate the partial derivative of $E$ with respect to $m$ and $c$, plug in $x,y ,m, c$ into it to get a numerical value. The direction of descent is given by the negative of the gradient. We move in the direction of the steepest slope, where the move size is proportional to the slope (high slope means a larger move). This is of course with the learning rate multiplied by it. $$ D_x=\frac 1n\sum_{i=0}^n2(y_i-(mx_i+c))(-x_i)=\frac{-2}n\sum_{i=0}^nx_i(y_i-(mx_i+c))$$Similarly, with respect to $c$,$$ D_c=\frac{-2}{n}\sum_{i=0}^n(y_i-(mx_i+c))$$This can be represented as the gradient
$$\nabla E=[\frac{\partial E}{\partial m},\frac{\partial E}{\partial c}]$$We then update the vector of parameters $\theta = [m,c]$ using the iterative process$$\theta^{\tau+1}=\theta^\tau-\eta\nabla E$$
##### Stochastic gradient descent
Unlike regular gradient descent, stochastic gradient descent just uses a randomly selected single datapoint $i$ from the training set, and uses that to update the model parameters. The process is given by $$\theta^{\tau+1}=\theta^{\tau}-\eta\nabla E^{i}$$where $i$ is the derivative using the single datapoint. Since the error is sum of squared errors, this works much faster in large datasets. Due to the noise in its updates, it can escape local minima. However, the updates can cause high variance and there might be no convergence to the minimum in some cases. 

##### Mini-Batch gradient descent
Combines the advantages of regular (batch) gradient descent and stochastic gradient descent, by splitting the dataset into small batches and calculating the gradient based on each batch rather than the entire dataset or a single sample. However, this introduces another hyperparameter - the batch size, that will have to be tweaked. The updates will be less noisy than in stochastic gradient descent, and is a suitable tradeoff between stochastic and batch gradient descent. 