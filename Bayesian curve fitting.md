The [[Probability#Conditional probability|Baye's theorem]] allows us to convert a prior probability into a posterior probability. We can use this to fit a probability distribution to the observed (training) data. We do this as opposed to regular least squares optimization as since this is probability based it is more likely to perform better out of sample. We are given the sample vectors of data $$X=[x_1, x_2, \dots, x_n]^T$$$$T=[t_1,t_2,\dots,t_n]^T$$where each $t_n$ is a datapoint and $x_n$ is the corresponding predictive variable. We are attempting to fit a curve to the data. The function for this curve is given by $$y(W,X)=\sum_{n=1}^Nw_nx^n$$where $W$ is the column vector of weights that we have to choose that best approximate the curve. Using baye's theorem, we can represent this problem in terms of probability$$\max _WP(W|D)$$where this is the probability of the weights being the value that best approximates the curve given the training data that we have - $D$. We want to find $W$ that maximizes this probability. Using the expansion of baye's theorem, we can infer that $$P(W|D)=\frac{P(D|W)P(W)}{P(D)}$$Since $P(D)$ is going to be constant as the data is the same irrespective of the $W$ that we choose, we do not need to worry about this term as we are choosing $W$ based on the comparison of probability. 

#### Probability distribution of $D$ given $W$
**Assumption:** The data $D$ is distributed normally around the value predicted by $y(W,X)$ 
![[Pasted image 20240903210056.png|400]]
We parameterize the normal distribution as follows:$$N(x|\mu, \sigma^2)=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp(\frac{1}{-2\sigma^2}(x-\mu)^2)$$This is essentially the assumption that the error terms are normally distributed.

where $\mu$ is the mean and $\sigma$ is the variance of the distribution. $\beta=1/\sigma^2$ is defined as the [[point estimation|precision]] (less $\sigma$ implies a more precise estimate). Using the idea of conditional probability, we can express $P(D|W)$ in terms of our assumption for each point:$$P(t_0|x_0, W,\beta)=N(t_0|y(x_0,W),\beta^{-1})$$$\beta$ is a hyperparameter that we will have to choose here for the best result from the machine learning exercise. We want to maximize the probability for $X$ given $W$. For all the points,$$P(T|X,W , \beta)=\prod_{n=1}^NN(t_n|y(x_n,W),\beta^{-1})$$This is the likelihood function. We take logs to convert the product into sums
$$\log P(T|X,W,\beta)=\frac{-\beta}{2}\sum_{n=1}^N\{y(x_n,W)-t_n\}^2+\frac N2\log\beta-\frac N2\log2\pi\tag 1$$We can drop the last 2 terms as they are independent of $W$, since we are choosing $W$ based on comparison. We can also take the negative of this function. Instead of maximizing the log likelihood, we minimize the negative log likelihood. $$\min_W \log P(T|X,W,\beta)=\frac{\beta}{2}\sum_{n=1}^N\{y(x_n, W)-t_n\}^2\tag 2$$
#### Probability distribution of $W$
We assume that $W$ is distributed normally. $\alpha$ is the precision of this distribution, it is another hyperparameter that we have to choose.
$$P(W|\alpha)=(\frac\alpha{2\pi})^{(M+1)/2}\exp\{-\frac\alpha2W^TW\}\tag3$$
#### Log likelihood function
By taking log of (3) and adding it to (2), we get the final negative log likelihood function$$\max_W\log P(W|D)\propto\min_W\frac{\beta}{2}\sum_{n=1}^N\{y(x_n, W)-t_n\}^2+\frac\alpha 2W^TW$$By minimizing the negative log likelihood we find the most probable $W$ given $D$. 

By maximizing 1, we can find the optimal $\beta$$$\frac{1}{\beta_{ML}}=\frac 1N\sum_{n=1}^N\{y(x_n,W_{ML}-t_n\}^2$$